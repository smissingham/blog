---
tags:
  - big-data
  - jupyter
  - pandas
  - spark
  - hdfs
  - wip
---
Ever wanted to work with a "big" data file without paying for rented cloud servers?

Ever thought to yourself "I wish I could just use SQL to query this CSV file like a database"?

Well, you can! With Apache Spark.


### Parquet Files

### Spark over Pandas?

### Installing Spark + HDFS + Jupyter

### Install JDK

#### Hadoop Dependencies

Create ENV var HADOOP_HOME and point it to the hadoop folder, one directory up from /bin
https://codeload.github.com/robguilarr/spark-winutils-3.3.1/zip/refs/heads/master

Copy the hadoop.dll file from /bin into C:/Windows/System32